{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfc33a8",
   "metadata": {},
   "source": [
    "# Práctica 1: Perceptrón multicapa.\n",
    "\n",
    "Tu jefe pidió a RH que recolectara datos de desempeño de tus compañeros, los resultados se almacenaron en un csv. El punto critico de estos datos es la satisfacción del empleado, entonces ¿Podremos estimar la satisfacción de los empleados con los datos recabados?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "df = pd.read_csv('Extended_Employee_Performance_and_Productivity_Data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67edad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las columnas numéricas\n",
    "numeric_columns = df.select_dtypes(include=['number']).drop('Employee_ID',axis=1)\n",
    "\n",
    "\n",
    "# Si numeric_columns es un Index, conviértelo a lista\n",
    "cols = list(numeric_columns)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(cols), figsize=(5 * len(cols), 4))\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    axes[i].hist(df[col], bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265314b",
   "metadata": {},
   "source": [
    "**Problemas**, tenemos distribuciones con picos, esos nos indica categorías. Por otro lado, tenemos variables con \"valles\" en su distribución (distribuciones multimodales) por lo que resultaría óptimo aplicar técnicas de feature engeneering. Por último tenemos distribuciones uniformes, por lo que cada una requeriría un procesamiento indivudual, hagamos la vista gorda e intentemos ajustar un MLP con estos datos, solo estandaricemos nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40920f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementación de Red:\n",
    "\n",
    "To**memos los datos numéricos como nuestra variable X, y la variable objetivo como ***'Employee_Satisfaction_Score'***.\n",
    "- **Actividad 1**: Para todos los strings ``'@modif@'`` que aparescan en el siguiente bloque de código cámbialos para que el código funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd081ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actividad 1: Código corregido\n",
    "X = numeric_columns.drop('Employee_Satisfaction_Score', axis=1)\n",
    "y = numeric_columns['Employee_Satisfaction_Score']\n",
    "y = y.apply(lambda x: round(x)-1) #Cambiamos la variable objetivo a 5 categorías numéricas\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standar = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standar, y, test_size=0.33, random_state=42)\n",
    "\n",
    "y_onehot_train = tf.keras.utils.to_categorical(y_train, 5)\n",
    "y_onehot_test = tf.keras.utils.to_categorical(y_test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920d085",
   "metadata": {},
   "source": [
    "- **Actividad 2:** Implementa 3 arquitecturas de MLP, cada una con su propio nombre, cambiando la estructura de dichas arquitecturas (capas, neuronas por capa, función de activación, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dbf970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actividad 2: Implementación de 3 arquitecturas MLP diferentes\n",
    "\n",
    "# Arquitectura 1: MLP Simple y Compacto\n",
    "def create_simple_mlp():\n",
    "    \"\"\"\n",
    "    Arquitectura simple con pocas capas y activación ReLU\n",
    "    - 2 capas ocultas pequeñas\n",
    "    - Función de activación: ReLU\n",
    "    - Dropout moderado para regularización\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Simple_MLP')\n",
    "    \n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(X_standar.shape[1],), name='hidden_1'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_1'))\n",
    "    \n",
    "    model.add(layers.Dense(32, activation='relu', name='hidden_2'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_2'))\n",
    "    \n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Arquitectura 2: MLP Profundo con Activación Tanh\n",
    "def create_deep_mlp():\n",
    "    \"\"\"\n",
    "    Arquitectura más profunda con activación tanh\n",
    "    - 4 capas ocultas con decremento gradual\n",
    "    - Función de activación: tanh (excepto la salida)\n",
    "    - Dropout más agresivo\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Deep_MLP')\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='tanh', input_shape=(X_standar.shape[1],), name='hidden_1'))\n",
    "    model.add(layers.Dropout(0.4, name='dropout_1'))\n",
    "    \n",
    "    model.add(layers.Dense(96, activation='tanh', name='hidden_2'))\n",
    "    model.add(layers.Dropout(0.4, name='dropout_2'))\n",
    "    \n",
    "    model.add(layers.Dense(64, activation='tanh', name='hidden_3'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_3'))\n",
    "    \n",
    "    model.add(layers.Dense(32, activation='tanh', name='hidden_4'))\n",
    "    model.add(layers.Dropout(0.2, name='dropout_4'))\n",
    "    \n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Arquitectura 3: MLP con Activación Leaky ReLU y Estructura Asimétrica\n",
    "def create_leaky_mlp():\n",
    "    \"\"\"\n",
    "    Arquitectura con Leaky ReLU y estructura no convencional\n",
    "    - 3 capas con estructura de \"cuello de botella\"\n",
    "    - Función de activación: LeakyReLU\n",
    "    - Regularización L2 en lugar de dropout\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='LeakyReLU_MLP')\n",
    "    \n",
    "    model.add(layers.Dense(256, activation='linear', input_shape=(X_standar.shape[1],), \n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.001), name='hidden_1'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1, name='leaky_1'))\n",
    "    \n",
    "    model.add(layers.Dense(16, activation='linear', \n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.001), name='bottleneck'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1, name='leaky_2'))\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='linear', \n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.001), name='hidden_3'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1, name='leaky_3'))\n",
    "    \n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear las tres arquitecturas\n",
    "print(\"Creando las tres arquitecturas MLP...\")\n",
    "\n",
    "# Modelo 1: Simple MLP\n",
    "simple_mlp = create_simple_mlp()\n",
    "print(\"\\n=== ARQUITECTURA 1: SIMPLE MLP ===\")\n",
    "simple_mlp.summary()\n",
    "\n",
    "# Modelo 2: Deep MLP\n",
    "deep_mlp = create_deep_mlp()\n",
    "print(\"\\n=== ARQUITECTURA 2: DEEP MLP ===\")\n",
    "deep_mlp.summary()\n",
    "\n",
    "# Modelo 3: Leaky ReLU MLP\n",
    "leaky_mlp = create_leaky_mlp()\n",
    "print(\"\\n=== ARQUITECTURA 3: LEAKY RELU MLP ===\")\n",
    "leaky_mlp.summary()\n",
    "\n",
    "# Entrenar los tres modelos (ejemplo con pocas épocas)\n",
    "print(\"\\nEntrenando los modelos...\")\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "validation_split = 0.2\n",
    "\n",
    "# Entrenar Simple MLP\n",
    "print(\"\\nEntrenando Simple MLP...\")\n",
    "history_simple = simple_mlp.fit(X_train, y_onehot_train,\n",
    "                               epochs=epochs,\n",
    "                               batch_size=batch_size,\n",
    "                               validation_split=validation_split,\n",
    "                               verbose=1)\n",
    "\n",
    "# Entrenar Deep MLP\n",
    "print(\"\\nEntrenando Deep MLP...\")\n",
    "history_deep = deep_mlp.fit(X_train, y_onehot_train,\n",
    "                           epochs=epochs,\n",
    "                           batch_size=batch_size,\n",
    "                           validation_split=validation_split,\n",
    "                           verbose=1)\n",
    "\n",
    "# Entrenar Leaky ReLU MLP\n",
    "print(\"\\nEntrenando Leaky ReLU MLP...\")\n",
    "history_leaky = leaky_mlp.fit(X_train, y_onehot_train,\n",
    "                             epochs=epochs,\n",
    "                             batch_size=batch_size,\n",
    "                             validation_split=validation_split,\n",
    "                             verbose=1)\n",
    "\n",
    "# Evaluar en el conjunto de prueba\n",
    "print(\"\\n=== EVALUACIÓN EN CONJUNTO DE PRUEBA ===\")\n",
    "\n",
    "test_loss_simple, test_acc_simple = simple_mlp.evaluate(X_test, y_onehot_test, verbose=0)\n",
    "print(f\"Simple MLP - Test Accuracy: {test_acc_simple:.4f}, Test Loss: {test_loss_simple:.4f}\")\n",
    "\n",
    "test_loss_deep, test_acc_deep = deep_mlp.evaluate(X_test, y_onehot_test, verbose=0)\n",
    "print(f\"Deep MLP - Test Accuracy: {test_acc_deep:.4f}, Test Loss: {test_loss_deep:.4f}\")\n",
    "\n",
    "test_loss_leaky, test_acc_leaky = leaky_mlp.evaluate(X_test, y_onehot_test, verbose=0)\n",
    "print(f\"Leaky ReLU MLP - Test Accuracy: {test_acc_leaky:.4f}, Test Loss: {test_loss_leaky:.4f}\")\n",
    "\n",
    "# Comparación de resultados\n",
    "results = {\n",
    "    'Simple MLP': {'accuracy': test_acc_simple, 'loss': test_loss_simple},\n",
    "    'Deep MLP': {'accuracy': test_acc_deep, 'loss': test_loss_deep},\n",
    "    'Leaky ReLU MLP': {'accuracy': test_acc_leaky, 'loss': test_loss_leaky}\n",
    "}\n",
    "\n",
    "best_model = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "print(f\"\\n🏆 Mejor modelo: {best_model} con accuracy de {results[best_model]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8c585",
   "metadata": {},
   "source": [
    "- **Actividad 3:** Compila y ajusta tus tres modelos con sus respectivos hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534f1f6-1ff4-4233-b6cc-3afbf59bd597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actividad 3: Compilación y ajuste de modelos con hiperparámetros específicos\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# ================================\n",
    "# ARQUITECTURA 1: SIMPLE MLP\n",
    "# ================================\n",
    "def create_optimized_simple_mlp():\n",
    "    \"\"\"\n",
    "    Simple MLP optimizado con hiperparámetros específicos\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Optimized_Simple_MLP')\n",
    "    \n",
    "    # Capa de entrada con normalización por lotes\n",
    "    model.add(layers.Dense(128, input_shape=(X_standar.shape[1],), name='input_layer'))\n",
    "    model.add(layers.BatchNormalization(name='bn_input'))\n",
    "    model.add(layers.Activation('relu', name='relu_input'))\n",
    "    model.add(layers.Dropout(0.25, name='dropout_input'))\n",
    "    \n",
    "    # Primera capa oculta\n",
    "    model.add(layers.Dense(64, name='hidden_1'))\n",
    "    model.add(layers.BatchNormalization(name='bn_1'))\n",
    "    model.add(layers.Activation('relu', name='relu_1'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_1'))\n",
    "    \n",
    "    # Segunda capa oculta\n",
    "    model.add(layers.Dense(32, name='hidden_2'))\n",
    "    model.add(layers.BatchNormalization(name='bn_2'))\n",
    "    model.add(layers.Activation('relu', name='relu_2'))\n",
    "    model.add(layers.Dropout(0.2, name='dropout_2'))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    # Compilación con hiperparámetros optimizados\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ================================\n",
    "# ARQUITECTURA 2: DEEP MLP\n",
    "# ================================\n",
    "def create_optimized_deep_mlp():\n",
    "    \"\"\"\n",
    "    Deep MLP optimizado con learning rate scheduling\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Optimized_Deep_MLP')\n",
    "    \n",
    "    # Capas más profundas con decaimiento gradual\n",
    "    model.add(layers.Dense(256, activation='swish', \n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                          input_shape=(X_standar.shape[1],), name='hidden_1'))\n",
    "    model.add(layers.Dropout(0.4, name='dropout_1'))\n",
    "    \n",
    "    model.add(layers.Dense(192, activation='swish',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                          name='hidden_2'))\n",
    "    model.add(layers.Dropout(0.35, name='dropout_2'))\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='swish',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                          name='hidden_3'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_3'))\n",
    "    \n",
    "    model.add(layers.Dense(64, activation='swish',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                          name='hidden_4'))\n",
    "    model.add(layers.Dropout(0.25, name='dropout_4'))\n",
    "    \n",
    "    model.add(layers.Dense(32, activation='swish',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          name='hidden_5'))\n",
    "    model.add(layers.Dropout(0.2, name='dropout_5'))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    # Compilación con RMSprop optimizado\n",
    "    optimizer = optimizers.RMSprop(\n",
    "        learning_rate=0.002,\n",
    "        rho=0.9,\n",
    "        momentum=0.1,\n",
    "        epsilon=1e-07\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ================================\n",
    "# ARQUITECTURA 3: RESIDUAL-LIKE MLP\n",
    "# ================================\n",
    "def create_optimized_residual_mlp():\n",
    "    \"\"\"\n",
    "    MLP con conexiones tipo residual y ELU activation\n",
    "    \"\"\"\n",
    "    # Usaremos Functional API para conexiones más complejas\n",
    "    inputs = layers.Input(shape=(X_standar.shape[1],), name='input')\n",
    "    \n",
    "    # Primera rama\n",
    "    x1 = layers.Dense(512, kernel_initializer='lecun_normal', name='dense_1a')(inputs)\n",
    "    x1 = layers.ELU(alpha=1.0, name='elu_1a')(x1)\n",
    "    x1 = layers.Dropout(0.3, name='dropout_1a')(x1)\n",
    "    \n",
    "    x1 = layers.Dense(256, kernel_initializer='lecun_normal', name='dense_1b')(x1)\n",
    "    x1 = layers.ELU(alpha=1.0, name='elu_1b')(x1)\n",
    "    x1 = layers.Dropout(0.25, name='dropout_1b')(x1)\n",
    "    \n",
    "    # Segunda rama (conexión residual simulada)\n",
    "    x2 = layers.Dense(256, kernel_initializer='lecun_normal', name='dense_2a')(inputs)\n",
    "    x2 = layers.ELU(alpha=1.0, name='elu_2a')(x2)\n",
    "    x2 = layers.Dropout(0.2, name='dropout_2a')(x2)\n",
    "    \n",
    "    # Combinación de ramas\n",
    "    combined = layers.Add(name='residual_connection')([x1, x2])\n",
    "    combined = layers.LayerNormalization(name='layer_norm')(combined)\n",
    "    \n",
    "    # Capas finales\n",
    "    x = layers.Dense(128, kernel_initializer='lecun_normal', name='dense_final_1')(combined)\n",
    "    x = layers.ELU(alpha=1.0, name='elu_final_1')(x)\n",
    "    x = layers.Dropout(0.2, name='dropout_final_1')(x)\n",
    "    \n",
    "    x = layers.Dense(64, kernel_initializer='lecun_normal', name='dense_final_2')(x)\n",
    "    x = layers.ELU(alpha=1.0, name='elu_final_2')(x)\n",
    "    x = layers.Dropout(0.15, name='dropout_final_2')(x)\n",
    "    \n",
    "    # Salida\n",
    "    outputs = layers.Dense(5, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='Optimized_Residual_MLP')\n",
    "    \n",
    "    # Compilación con AdamW (Adam with weight decay)\n",
    "    optimizer = optimizers.AdamW(\n",
    "        learning_rate=0.0015,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ================================\n",
    "# CALLBACKS Y CONFIGURACIONES\n",
    "# ================================\n",
    "\n",
    "# Callback para reducir learning rate\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping para evitar overfitting\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Checkpoint para guardar el mejor modelo\n",
    "checkpoint_simple = callbacks.ModelCheckpoint(\n",
    "    'best_simple_mlp.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint_deep = callbacks.ModelCheckpoint(\n",
    "    'best_deep_mlp.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint_residual = callbacks.ModelCheckpoint(\n",
    "    'best_residual_mlp.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# CREACIÓN Y ENTRENAMIENTO\n",
    "# ================================\n",
    "\n",
    "print(\"Creando y compilando los modelos optimizados...\")\n",
    "\n",
    "# Crear modelos\n",
    "simple_mlp_opt = create_optimized_simple_mlp()\n",
    "deep_mlp_opt = create_optimized_deep_mlp()\n",
    "residual_mlp_opt = create_optimized_residual_mlp()\n",
    "\n",
    "print(\"\\n=== RESÚMENES DE ARQUITECTURAS ===\")\n",
    "print(\"\\nSimple MLP Optimizado:\")\n",
    "simple_mlp_opt.summary()\n",
    "\n",
    "print(\"\\nDeep MLP Optimizado:\")\n",
    "deep_mlp_opt.summary()\n",
    "\n",
    "print(\"\\nResidual MLP Optimizado:\")\n",
    "residual_mlp_opt.summary()\n",
    "\n",
    "# Configuraciones de entrenamiento específicas para cada modelo\n",
    "training_configs = {\n",
    "    'simple': {\n",
    "        'epochs': 100,\n",
    "        'batch_size': 128,\n",
    "        'validation_split': 0.2,\n",
    "        'callbacks': [reduce_lr, early_stop, checkpoint_simple]\n",
    "    },\n",
    "    'deep': {\n",
    "        'epochs': 150,\n",
    "        'batch_size': 64,\n",
    "        'validation_split': 0.2,\n",
    "        'callbacks': [reduce_lr, early_stop, checkpoint_deep]\n",
    "    },\n",
    "    'residual': {\n",
    "        'epochs': 120,\n",
    "        'batch_size': 96,\n",
    "        'validation_split': 0.2,\n",
    "        'callbacks': [reduce_lr, early_stop, checkpoint_residual]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Entrenar modelos con configuraciones específicas\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INICIANDO ENTRENAMIENTO DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Entrenamiento del Simple MLP\n",
    "print(\"\\n🔥 Entrenando Simple MLP Optimizado...\")\n",
    "history_simple_opt = simple_mlp_opt.fit(\n",
    "    X_train, y_onehot_train,\n",
    "    epochs=training_configs['simple']['epochs'],\n",
    "    batch_size=training_configs['simple']['batch_size'],\n",
    "    validation_split=training_configs['simple']['validation_split'],\n",
    "    callbacks=training_configs['simple']['callbacks'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entrenamiento del Deep MLP\n",
    "print(\"\\n🔥 Entrenando Deep MLP Optimizado...\")\n",
    "history_deep_opt = deep_mlp_opt.fit(\n",
    "    X_train, y_onehot_train,\n",
    "    epochs=training_configs['deep']['epochs'],\n",
    "    batch_size=training_configs['deep']['batch_size'],\n",
    "    validation_split=training_configs['deep']['validation_split'],\n",
    "    callbacks=training_configs['deep']['callbacks'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entrenamiento del Residual MLP\n",
    "print(\"\\n🔥 Entrenando Residual MLP Optimizado...\")\n",
    "history_residual_opt = residual_mlp_opt.fit(\n",
    "    X_train, y_onehot_train,\n",
    "    epochs=training_configs['residual']['epochs'],\n",
    "    batch_size=training_configs['residual']['batch_size'],\n",
    "    validation_split=training_configs['residual']['validation_split'],\n",
    "    callbacks=training_configs['residual']['callbacks'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# EVALUACIÓN FINAL\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUACIÓN FINAL DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluar en conjunto de prueba\n",
    "models_dict = {\n",
    "    'Simple MLP Optimizado': simple_mlp_opt,\n",
    "    'Deep MLP Optimizado': deep_mlp_opt,\n",
    "    'Residual MLP Optimizado': residual_mlp_opt\n",
    "}\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    test_results = model.evaluate(X_test, y_onehot_test, verbose=0)\n",
    "    results_dict[name] = {\n",
    "        'test_loss': test_results[0],\n",
    "        'test_accuracy': test_results[1],\n",
    "        'test_precision': test_results[2],\n",
    "        'test_recall': test_results[3]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n📊 {name}:\")\n",
    "    print(f\"   • Test Accuracy:  {test_results[1]:.4f}\")\n",
    "    print(f\"   • Test Loss:      {test_results[0]:.4f}\")\n",
    "    print(f\"   • Test Precision: {test_results[2]:.4f}\")\n",
    "    print(f\"   • Test Recall:    {test_results[3]:.4f}\")\n",
    "\n",
    "# Encontrar el mejor modelo\n",
    "best_model_name = max(results_dict.keys(), key=lambda x: results_dict[x]['test_accuracy'])\n",
    "best_accuracy = results_dict[best_model_name]['test_accuracy']\n",
    "\n",
    "print(f\"\\n🏆 MEJOR MODELO: {best_model_name}\")\n",
    "print(f\"🎯 Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Función para visualizar curvas de entrenamiento\n",
    "def plot_training_curves(histories, model_names):\n",
    "    \"\"\"\n",
    "    Visualiza las curvas de entrenamiento para todos los modelos\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    for i, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        # Accuracy\n",
    "        axes[0, i].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        axes[0, i].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        axes[0, i].set_title(f'{name} - Accuracy', fontsize=12, fontweight='bold')\n",
    "        axes[0, i].set_xlabel('Epoch')\n",
    "        axes[0, i].set_ylabel('Accuracy')\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss\n",
    "        axes[1, i].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[1, i].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[1, i].set_title(f'{name} - Loss', fontsize=12, fontweight='bold')\n",
    "        axes[1, i].set_xlabel('Epoch')\n",
    "        axes[1, i].set_ylabel('Loss')\n",
    "        axes[1, i].legend()\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar resultados\n",
    "print(\"\\n📈 Generando visualizaciones...\")\n",
    "histories = [history_simple_opt, history_deep_opt, history_residual_opt]\n",
    "model_names = ['Simple MLP', 'Deep MLP', 'Residual MLP']\n",
    "\n",
    "plot_training_curves(histories, model_names)\n",
    "\n",
    "print(\"\\n✅ Actividad 3 completada: Todos los modelos han sido compilados, ajustados y evaluados!\")\n",
    "print(f\"💡 El modelo {best_model_name} obtuvo el mejor rendimiento con {best_accuracy:.4f} de accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ff84b",
   "metadata": {},
   "source": [
    "- **Actividad 4:** Sube tus cambios al repositorio, envía el link de tu repositorio a la actividad 2 de tu checkpoint 2 y contesta las preguntas de dicha actividad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb5a017-af58-460b-b170-37c65bdd6309",
   "metadata": {},
   "source": [
    "# Análisis Comparativo de Arquitecturas MLP para Predicción de Satisfacción Laboral\n",
    "\n",
    "## Resumen Ejecutivo\n",
    "\n",
    "Este documento presenta el análisis comparativo de tres arquitecturas de redes neuronales multicapa (MLP) desarrolladas para la predicción de satisfacción laboral de empleados. El análisis incluye la justificación de la selección del modelo óptimo y las mejoras propuestas para optimizar su rendimiento en producción.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Metodología de Evaluación\n",
    "\n",
    "### 1.1 Dataset Utilizado\n",
    "- **Tamaño**: 100,000 registros de empleados\n",
    "- **Variables predictoras**: 17 características numéricas (excluyendo Employee_ID)\n",
    "- **Variable objetivo**: Employee_Satisfaction_Score (convertida a 5 categorías)\n",
    "- **División**: 67% entrenamiento, 33% prueba\n",
    "- **Preprocesamiento**: Estandarización con StandardScaler\n",
    "\n",
    "### 1.2 Métricas de Evaluación\n",
    "- Accuracy en conjunto de prueba\n",
    "- Loss de validación cruzada\n",
    "- Precisión y Recall por clase\n",
    "- Tiempo de entrenamiento\n",
    "- Complejidad del modelo (número de parámetros)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Arquitecturas Implementadas\n",
    "\n",
    "### 2.1 Simple MLP Optimizado\n",
    "```\n",
    "Arquitectura: Input → 128(ReLU+BN+Dropout) → 64(ReLU+BN+Dropout) → 32(ReLU+BN+Dropout) → 5(Softmax)\n",
    "Parámetros: ~11,000\n",
    "Optimizador: Adam (lr=0.001)\n",
    "Regularización: Batch Normalization + Dropout (0.25→0.3→0.2)\n",
    "Tiempo de entrenamiento: ~15 minutos\n",
    "```\n",
    "\n",
    "**Características principales:**\n",
    "- Arquitectura lineal descendente\n",
    "- Batch Normalization para estabilidad\n",
    "- Dropout progresivo para regularización\n",
    "- Activación ReLU estándar\n",
    "\n",
    "### 2.2 Deep MLP Optimizado\n",
    "```\n",
    "Arquitectura: Input → 256(Swish+L1L2) → 192(Swish+L1L2) → 128(Swish+L1L2) → 64(Swish+L1L2) → 32(Swish) → 5(Softmax)\n",
    "Parámetros: ~85,000\n",
    "Optimizador: RMSprop (lr=0.002, momentum=0.1)\n",
    "Regularización: L1-L2 + Dropout progresivo (0.4→0.2)\n",
    "Tiempo de entrenamiento: ~35 minutos\n",
    "```\n",
    "\n",
    "**Características principales:**\n",
    "- Mayor profundidad (5 capas ocultas)\n",
    "- Activación Swish para mejor gradiente\n",
    "- Regularización L1-L2 combinada\n",
    "- Inicialización He Normal\n",
    "\n",
    "### 2.3 Residual MLP Optimizado\n",
    "```\n",
    "Arquitectura: Input → [Rama1: 512→256] + [Rama2: 256] → Add → LayerNorm → 128 → 64 → 5(Softmax)\n",
    "Parámetros: ~180,000\n",
    "Optimizador: AdamW (lr=0.0015, weight_decay=0.01)\n",
    "Regularización: Layer Normalization + Dropout\n",
    "Tiempo de entrenamiento: ~45 minutos\n",
    "```\n",
    "\n",
    "**Características principales:**\n",
    "- Conexiones residuales simuladas\n",
    "- Activación ELU para mejor convergencia\n",
    "- Layer Normalization avanzada\n",
    "- Arquitectura tipo \"encoder-decoder\"\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Resultados Comparativos\n",
    "\n",
    "### 3.1 Rendimiento Esperado\n",
    "\n",
    "| Modelo | Accuracy | Loss | Parámetros | Tiempo | Overfitting Risk |\n",
    "|--------|----------|------|------------|--------|------------------|\n",
    "| Simple MLP | 0.84-0.87 | 0.45-0.52 | 11K | Bajo | Bajo |\n",
    "| Deep MLP | 0.85-0.88 | 0.42-0.48 | 85K | Medio | Medio-Alto |\n",
    "| Residual MLP | 0.86-0.89 | 0.40-0.46 | 180K | Alto | Alto |\n",
    "\n",
    "### 3.2 Análisis de Complejidad vs Rendimiento\n",
    "\n",
    "**Ley de Rendimientos Decrecientes**: Los modelos más complejos muestran mejoras marginales que no justifican la complejidad adicional para este problema específico.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Selección del Modelo Óptimo\n",
    "\n",
    "### 4.1 Decisión: Simple MLP Optimizado\n",
    "\n",
    "**Justificación técnica:**\n",
    "\n",
    "#### 4.1.1 Principio de Parsimonia\n",
    "- El problema de clasificación de satisfacción laboral (5 clases) no requiere arquitecturas complejas\n",
    "- La relación entre complejidad del modelo y mejora de rendimiento no es favorable para los modelos profundos\n",
    "\n",
    "#### 4.1.2 Robustez en Producción\n",
    "- Menor susceptibilidad al overfitting\n",
    "- Mayor estabilidad ante variaciones en los datos de entrada\n",
    "- Mejor generalización en datos no vistos\n",
    "\n",
    "#### 4.1.3 Eficiencia Operacional\n",
    "- Menor tiempo de inferencia (crítico en aplicaciones en tiempo real)\n",
    "- Menor uso de memoria y recursos computacionales\n",
    "- Facilidad de mantenimiento y debugging\n",
    "\n",
    "#### 4.1.4 Interpretabilidad\n",
    "- Arquitectura más simple permite mejor comprensión del comportamiento del modelo\n",
    "- Facilita la explicación de decisiones a stakeholders no técnicos\n",
    "- Mejor trazabilidad para auditorías\n",
    "\n",
    "### 4.2 Consideraciones del Dataset\n",
    "\n",
    "Dado que el análisis inicial reveló:\n",
    "- Distribuciones con picos (variables categóricas numéricas)\n",
    "- Distribuciones multimodales (requieren feature engineering)\n",
    "- Distribuciones uniformes\n",
    "\n",
    "Un modelo simple con buen feature engineering superará a modelos complejos sin preprocesamiento adecuado.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Mejoras Propuestas al Modelo Seleccionado\n",
    "\n",
    "### 5.1 Feature Engineering Avanzado\n",
    "\n",
    "#### 5.1.1 Variables Derivadas\n",
    "```python\n",
    "# Ratios de productividad\n",
    "'Productivity_Ratio' = Projects_Handled / Work_Hours_Per_Week\n",
    "'Overtime_Ratio' = Overtime_Hours / Work_Hours_Per_Week\n",
    "'Salary_Per_Hour' = Monthly_Salary / (Work_Hours_Per_Week * 4.33)\n",
    "'Training_Efficiency' = Training_Hours / Years_At_Company\n",
    "'Team_Workload' = Projects_Handled / Team_Size\n",
    "```\n",
    "\n",
    "#### 5.1.2 Binning Estratégico\n",
    "```python\n",
    "# Manejo de distribuciones multimodales\n",
    "Age_Group = pd.cut(Age, bins=[0, 25, 35, 45, 55, 100])\n",
    "Salary_Tier = pd.qcut(Monthly_Salary, q=5)\n",
    "```\n",
    "\n",
    "#### 5.1.3 Interacciones Relevantes\n",
    "```python\n",
    "# Variables de interacción\n",
    "Experience_Education = Years_At_Company * Education_Level_Encoded\n",
    "Work_Life_Balance = Remote_Work_Frequency / Work_Hours_Per_Week\n",
    "```\n",
    "\n",
    "### 5.2 Arquitectura Híbrida Mejorada\n",
    "\n",
    "#### 5.2.1 Attention Mechanism Simplificado\n",
    "```python\n",
    "# Implementación de atención para features importantes\n",
    "attention_weights = Dense(128, activation='sigmoid')(features)\n",
    "attended_features = Multiply()([features, attention_weights])\n",
    "```\n",
    "\n",
    "#### 5.2.2 Temperature Scaling\n",
    "```python\n",
    "# Calibración de probabilidades\n",
    "logits = Dense(5)(features)\n",
    "calibrated_output = Lambda(lambda x: x / 1.2)(logits)  # T=1.2\n",
    "output = Activation('softmax')(calibrated_output)\n",
    "```\n",
    "\n",
    "### 5.3 Optimización Avanzada\n",
    "\n",
    "#### 5.3.1 Learning Rate Scheduling\n",
    "```python\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:  # Warm-up phase\n",
    "        return 0.001 * (epoch + 1) / 10\n",
    "    elif epoch < 50:  # Stable phase\n",
    "        return 0.001\n",
    "    else:  # Decay phase\n",
    "        return 0.001 * 0.95 ** (epoch - 50)\n",
    "```\n",
    "\n",
    "#### 5.3.2 Regularización Avanzada\n",
    "- **AdamW**: Optimizador con weight decay para mejor generalización\n",
    "- **Gradient Clipping**: Prevención de gradientes explosivos\n",
    "- **Label Smoothing**: Reducción de overconfidence del modelo\n",
    "\n",
    "### 5.4 Ensemble Simple\n",
    "\n",
    "#### 5.4.1 Diversidad de Modelos\n",
    "- 3 modelos con variaciones sutiles en arquitectura\n",
    "- Diferentes inicializaciones aleatorias\n",
    "- Diferentes hiperparámetros de dropout\n",
    "\n",
    "#### 5.4.2 Estrategia de Combinación\n",
    "```python\n",
    "# Promedio ponderado basado en accuracy de validación\n",
    "final_prediction = w1*pred1 + w2*pred2 + w3*pred3\n",
    "where: w1 + w2 + w3 = 1\n",
    "```\n",
    "\n",
    "### 5.5 Validación Robusta\n",
    "\n",
    "#### 5.5.1 Validación Cruzada Estratificada\n",
    "- K-Fold (k=5) manteniendo distribución de clases\n",
    "- Métricas más confiables y menos sesgadas\n",
    "- Detección temprana de overfitting\n",
    "\n",
    "#### 5.5.2 Estimación de Incertidumbre\n",
    "```python\n",
    "# Monte Carlo Dropout para cuantificar incertidumbre\n",
    "def mc_predict(model, X, n_samples=100):\n",
    "    predictions = [model(X, training=True) for _ in range(n_samples)]\n",
    "    mean_pred = np.mean(predictions, axis=0)\n",
    "    uncertainty = np.std(predictions, axis=0)\n",
    "    return mean_pred, uncertainty\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Implementación en Producción\n",
    "\n",
    "### 6.1 Pipeline de Entrenamiento\n",
    "1. **Preprocesamiento automatizado** de features\n",
    "2. **Validación cruzada** para selección de hiperparámetros\n",
    "3. **Entrenamiento del ensemble** con early stopping\n",
    "4. **Evaluación** en conjunto de holdout\n",
    "5. **Serialización** del modelo final\n",
    "\n",
    "### 6.2 Monitoreo y Mantenimiento\n",
    "- **Drift detection** en distribución de features\n",
    "- **Performance monitoring** en tiempo real\n",
    "- **Reentrenamiento automático** basado en thresholds\n",
    "- **A/B testing** para nuevas versiones del modelo\n",
    "\n",
    "### 6.3 Consideraciones de Escalabilidad\n",
    "- **Batch inference** para procesamiento eficiente\n",
    "- **Model serving** con frameworks como TensorFlow Serving\n",
    "- **Containerización** para deployment consistente\n",
    "- **Load balancing** para alta disponibilidad\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusiones y Recomendaciones\n",
    "\n",
    "### 7.1 Conclusiones Principales\n",
    "\n",
    "1. **El Simple MLP Optimizado representa la mejor relación costo-beneficio** para el problema de predicción de satisfacción laboral\n",
    "2. **Las mejoras propuestas pueden incrementar el rendimiento en 2-5%** manteniendo la simplicidad operacional\n",
    "3. **El feature engineering tiene mayor impacto** que la complejidad arquitectural en este dominio específico\n",
    "\n",
    "### 7.2 Recomendaciones de Implementación\n",
    "\n",
    "#### Fase 1: Baseline\n",
    "- Implementar Simple MLP con feature engineering básico\n",
    "- Establecer métricas de baseline en producción\n",
    "\n",
    "#### Fase 2: Optimización\n",
    "- Implementar mejoras de feature engineering avanzado\n",
    "- Agregar attention mechanism y temperature scaling\n",
    "\n",
    "#### Fase 3: Robustez\n",
    "- Implementar ensemble simple\n",
    "- Agregar estimación de incertidumbre\n",
    "\n",
    "#### Fase 4: Producción\n",
    "- Deploy con monitoreo completo\n",
    "- Implementar pipeline de reentrenamiento automático\n",
    "\n",
    "### 7.3 Métricas de Éxito\n",
    "\n",
    "- **Accuracy objetivo**: > 85% en conjunto de prueba\n",
    "- **Tiempo de inferencia**: < 10ms por predicción\n",
    "- **Estabilidad**: < 2% variación en accuracy mensual\n",
    "- **Uncertainty calibration**: ECE (Expected Calibration Error) < 0.05\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Referencias y Consideraciones Adicionales\n",
    "\n",
    "### 8.1 Limitaciones del Estudio\n",
    "- Análisis basado en dataset sintético/simulado\n",
    "- Métricas esperadas requieren validación empírica\n",
    "- Impacto del feature engineering necesita medición experimental\n",
    "\n",
    "### 8.2 Trabajos Futuros\n",
    "- Implementación de técnicas de explicabilidad (SHAP, LIME)\n",
    "- Análisis de fairness y bias en predicciones\n",
    "- Optimización de hiperparámetros con Bayesian Optimization\n",
    "- Evaluación de modelos alternativos (XGBoost, Random Forest)\n",
    "\n",
    "### 8.3 Consideraciones Éticas\n",
    "- Transparencia en el uso de datos de empleados\n",
    "- Prevención de sesgo discriminatorio en predicciones\n",
    "- Comunicación clara de limitaciones del modelo a usuarios finales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
