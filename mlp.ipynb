{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfc33a8",
   "metadata": {},
   "source": [
    "# Pr√°ctica 1: Perceptr√≥n multicapa.\n",
    "\n",
    "Tu jefe pidi√≥ a RH que recolectara datos de desempe√±o de tus compa√±eros, los resultados se almacenaron en un csv. El punto critico de estos datos es la satisfacci√≥n del empleado, entonces ¬øPodremos estimar la satisfacci√≥n de los empleados con los datos recabados?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "df = pd.read_csv('Extended_Employee_Performance_and_Productivity_Data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67edad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las columnas num√©ricas\n",
    "numeric_columns = df.select_dtypes(include=['number']).drop('Employee_ID',axis=1)\n",
    "\n",
    "\n",
    "# Si numeric_columns es un Index, convi√©rtelo a lista\n",
    "cols = list(numeric_columns)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(cols), figsize=(5 * len(cols), 4))\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    axes[i].hist(df[col], bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265314b",
   "metadata": {},
   "source": [
    "**Problemas**, tenemos distribuciones con picos, esos nos indica categor√≠as. Por otro lado, tenemos variables con \"valles\" en su distribuci√≥n (distribuciones multimodales) por lo que resultar√≠a √≥ptimo aplicar t√©cnicas de feature engeneering. Por √∫ltimo tenemos distribuciones uniformes, por lo que cada una requerir√≠a un procesamiento indivudual, hagamos la vista gorda e intentemos ajustar un MLP con estos datos, solo estandaricemos nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40920f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementaci√≥n de Red:\n",
    "\n",
    "To**memos los datos num√©ricos como nuestra variable X, y la variable objetivo como ***'Employee_Satisfaction_Score'***.\n",
    "- **Actividad 1**: Para todos los strings ``'@modif@'`` que aparescan en el siguiente bloque de c√≥digo c√°mbialos para que el c√≥digo funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd081ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actividad 1: C√≥digo corregido\n",
    "X = numeric_columns.drop('Employee_Satisfaction_Score', axis=1)\n",
    "y = numeric_columns['Employee_Satisfaction_Score']\n",
    "y = y.apply(lambda x: round(x)-1) #Cambiamos la variable objetivo a 5 categor√≠as num√©ricas\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standar = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standar, y, test_size=0.33, random_state=42)\n",
    "\n",
    "y_onehot_train = tf.keras.utils.to_categorical(y_train, 5)\n",
    "y_onehot_test = tf.keras.utils.to_categorical(y_test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920d085",
   "metadata": {},
   "source": [
    "- **Actividad 2:** Implementa 3 arquitecturas de MLP, cada una con su propio nombre, cambiando la estructura de dichas arquitecturas (capas, neuronas por capa, funci√≥n de activaci√≥n, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dbf970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actividad 2: Implementaci√≥n de 3 arquitecturas MLP diferentes\n",
    "\n",
    "# Arquitectura 1: MLP Simple y Compacto\n",
    "def create_simple_mlp():\n",
    "    \"\"\"\n",
    "    Arquitectura simple con pocas capas y activaci√≥n ReLU\n",
    "    - 2 capas ocultas peque√±as\n",
    "    - Funci√≥n de activaci√≥n: ReLU\n",
    "    - Dropout moderado para regularizaci√≥n\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Simple_MLP')\n",
    "    \n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(X_standar.shape[1],), name='hidden_1'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_1'))\n",
    "    \n",
    "    model.add(layers.Dense(32, activation='relu', name='hidden_2'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_2'))\n",
    "    \n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Arquitectura 2: MLP Profundo con Activaci√≥n Tanh\n",
    "def create_deep_mlp():\n",
    "    \"\"\"\n",
    "    Arquitectura m√°s profunda con activaci√≥n tanh\n",
    "    - 4 capas ocultas con decremento gradual\n",
    "    - Funci√≥n de activaci√≥n: tanh (excepto la salida)\n",
    "    - Dropout m√°s agresivo\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Deep_MLP')\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='tanh', input_shape=(X_standar.shape[1],), name='hidden_1'))\n",
    "    model.add(layers.Dropout(0.4, name='dropout_1'))\n",
    "    \n",
    "    model.add(layers.Dense(96, activation='tanh', name='hidden_2'))\n",
    "    model.add(layers.Dropout(0.4, name='dropout_2'))\n",
    "    \n",
    "    model.add(layers.Dense(64, activation='tanh', name='hidden_3'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_3'))\n",
    "    \n",
    "    model.add(layers.Dense(32, activation='tanh', name='hidden_4'))\n",
    "    model.add(layers.Dropout(0.2, name='dropout_4'))\n",
    "    \n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Arquitectura 3: MLP con Activaci√≥n Leaky ReLU y Estructura Asim√©trica\n",
    "def create_leaky_mlp():\n",
    "    \"\"\"\n",
    "    Arquitectura con Leaky ReLU y estructura no convencional\n",
    "    - 3 capas con estructura de \"cuello de botella\"\n",
    "    - Funci√≥n de activaci√≥n: LeakyReLU\n",
    "    - Regularizaci√≥n L2 en lugar de dropout\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='LeakyReLU_MLP')\n",
    "    \n",
    "    model.add(layers.Dense(256, activation='linear', input_shape=(X_standar.shape[1],), \n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.001), name='hidden_1'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1, name='leaky_1'))\n",
    "    \n",
    "    model.add(layers.Dense(16, activation='linear', \n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.001), name='bottleneck'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1, name='leaky_2'))\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='linear', \n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.001), name='hidden_3'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.1, name='leaky_3'))\n",
    "    \n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear las tres arquitecturas\n",
    "print(\"Creando las tres arquitecturas MLP...\")\n",
    "\n",
    "# Modelo 1: Simple MLP\n",
    "simple_mlp = create_simple_mlp()\n",
    "print(\"\\n=== ARQUITECTURA 1: SIMPLE MLP ===\")\n",
    "simple_mlp.summary()\n",
    "\n",
    "# Modelo 2: Deep MLP\n",
    "deep_mlp = create_deep_mlp()\n",
    "print(\"\\n=== ARQUITECTURA 2: DEEP MLP ===\")\n",
    "deep_mlp.summary()\n",
    "\n",
    "# Modelo 3: Leaky ReLU MLP\n",
    "leaky_mlp = create_leaky_mlp()\n",
    "print(\"\\n=== ARQUITECTURA 3: LEAKY RELU MLP ===\")\n",
    "leaky_mlp.summary()\n",
    "\n",
    "# Entrenar los tres modelos (ejemplo con pocas √©pocas)\n",
    "print(\"\\nEntrenando los modelos...\")\n",
    "\n",
    "# Configuraci√≥n de entrenamiento\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "validation_split = 0.2\n",
    "\n",
    "# Entrenar Simple MLP\n",
    "print(\"\\nEntrenando Simple MLP...\")\n",
    "history_simple = simple_mlp.fit(X_train, y_onehot_train,\n",
    "                               epochs=epochs,\n",
    "                               batch_size=batch_size,\n",
    "                               validation_split=validation_split,\n",
    "                               verbose=1)\n",
    "\n",
    "# Entrenar Deep MLP\n",
    "print(\"\\nEntrenando Deep MLP...\")\n",
    "history_deep = deep_mlp.fit(X_train, y_onehot_train,\n",
    "                           epochs=epochs,\n",
    "                           batch_size=batch_size,\n",
    "                           validation_split=validation_split,\n",
    "                           verbose=1)\n",
    "\n",
    "# Entrenar Leaky ReLU MLP\n",
    "print(\"\\nEntrenando Leaky ReLU MLP...\")\n",
    "history_leaky = leaky_mlp.fit(X_train, y_onehot_train,\n",
    "                             epochs=epochs,\n",
    "                             batch_size=batch_size,\n",
    "                             validation_split=validation_split,\n",
    "                             verbose=1)\n",
    "\n",
    "# Evaluar en el conjunto de prueba\n",
    "print(\"\\n=== EVALUACI√ìN EN CONJUNTO DE PRUEBA ===\")\n",
    "\n",
    "test_loss_simple, test_acc_simple = simple_mlp.evaluate(X_test, y_onehot_test, verbose=0)\n",
    "print(f\"Simple MLP - Test Accuracy: {test_acc_simple:.4f}, Test Loss: {test_loss_simple:.4f}\")\n",
    "\n",
    "test_loss_deep, test_acc_deep = deep_mlp.evaluate(X_test, y_onehot_test, verbose=0)\n",
    "print(f\"Deep MLP - Test Accuracy: {test_acc_deep:.4f}, Test Loss: {test_loss_deep:.4f}\")\n",
    "\n",
    "test_loss_leaky, test_acc_leaky = leaky_mlp.evaluate(X_test, y_onehot_test, verbose=0)\n",
    "print(f\"Leaky ReLU MLP - Test Accuracy: {test_acc_leaky:.4f}, Test Loss: {test_loss_leaky:.4f}\")\n",
    "\n",
    "# Comparaci√≥n de resultados\n",
    "results = {\n",
    "    'Simple MLP': {'accuracy': test_acc_simple, 'loss': test_loss_simple},\n",
    "    'Deep MLP': {'accuracy': test_acc_deep, 'loss': test_loss_deep},\n",
    "    'Leaky ReLU MLP': {'accuracy': test_acc_leaky, 'loss': test_loss_leaky}\n",
    "}\n",
    "\n",
    "best_model = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "print(f\"\\nüèÜ Mejor modelo: {best_model} con accuracy de {results[best_model]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8c585",
   "metadata": {},
   "source": [
    "- **Actividad 3:** Compila y ajusta tus tres modelos con sus respectivos hiperpar√°metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534f1f6-1ff4-4233-b6cc-3afbf59bd597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actividad 3: Compilaci√≥n y ajuste de modelos con hiperpar√°metros espec√≠ficos\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# ================================\n",
    "# ARQUITECTURA 1: SIMPLE MLP\n",
    "# ================================\n",
    "def create_optimized_simple_mlp():\n",
    "    \"\"\"\n",
    "    Simple MLP optimizado con hiperpar√°metros espec√≠ficos\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Optimized_Simple_MLP')\n",
    "    \n",
    "    # Capa de entrada con normalizaci√≥n por lotes\n",
    "    model.add(layers.Dense(128, input_shape=(X_standar.shape[1],), name='input_layer'))\n",
    "    model.add(layers.BatchNormalization(name='bn_input'))\n",
    "    model.add(layers.Activation('relu', name='relu_input'))\n",
    "    model.add(layers.Dropout(0.25, name='dropout_input'))\n",
    "    \n",
    "    # Primera capa oculta\n",
    "    model.add(layers.Dense(64, name='hidden_1'))\n",
    "    model.add(layers.BatchNormalization(name='bn_1'))\n",
    "    model.add(layers.Activation('relu', name='relu_1'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_1'))\n",
    "    \n",
    "    # Segunda capa oculta\n",
    "    model.add(layers.Dense(32, name='hidden_2'))\n",
    "    model.add(layers.BatchNormalization(name='bn_2'))\n",
    "    model.add(layers.Activation('relu', name='relu_2'))\n",
    "    model.add(layers.Dropout(0.2, name='dropout_2'))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    # Compilaci√≥n con hiperpar√°metros optimizados\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ================================\n",
    "# ARQUITECTURA 2: DEEP MLP\n",
    "# ================================\n",
    "def create_optimized_deep_mlp():\n",
    "    \"\"\"\n",
    "    Deep MLP optimizado con learning rate scheduling\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Optimized_Deep_MLP')\n",
    "    \n",
    "    # Capas m√°s profundas con decaimiento gradual\n",
    "    model.add(layers.Dense(256, activation='swish', \n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                          input_shape=(X_standar.shape[1],), name='hidden_1'))\n",
    "    model.add(layers.Dropout(0.4, name='dropout_1'))\n",
    "    \n",
    "    model.add(layers.Dense(192, activation='swish',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                          name='hidden_2'))\n",
    "    model.add(layers.Dropout(0.35, name='dropout_2'))\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='swish',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                          name='hidden_3'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout_3'))\n",
    "    \n",
    "    model.add(layers.Dense(64, activation='swish',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                          name='hidden_4'))\n",
    "    model.add(layers.Dropout(0.25, name='dropout_4'))\n",
    "    \n",
    "    model.add(layers.Dense(32, activation='swish',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          name='hidden_5'))\n",
    "    model.add(layers.Dropout(0.2, name='dropout_5'))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(layers.Dense(5, activation='softmax', name='output'))\n",
    "    \n",
    "    # Compilaci√≥n con RMSprop optimizado\n",
    "    optimizer = optimizers.RMSprop(\n",
    "        learning_rate=0.002,\n",
    "        rho=0.9,\n",
    "        momentum=0.1,\n",
    "        epsilon=1e-07\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ================================\n",
    "# ARQUITECTURA 3: RESIDUAL-LIKE MLP\n",
    "# ================================\n",
    "def create_optimized_residual_mlp():\n",
    "    \"\"\"\n",
    "    MLP con conexiones tipo residual y ELU activation\n",
    "    \"\"\"\n",
    "    # Usaremos Functional API para conexiones m√°s complejas\n",
    "    inputs = layers.Input(shape=(X_standar.shape[1],), name='input')\n",
    "    \n",
    "    # Primera rama\n",
    "    x1 = layers.Dense(512, kernel_initializer='lecun_normal', name='dense_1a')(inputs)\n",
    "    x1 = layers.ELU(alpha=1.0, name='elu_1a')(x1)\n",
    "    x1 = layers.Dropout(0.3, name='dropout_1a')(x1)\n",
    "    \n",
    "    x1 = layers.Dense(256, kernel_initializer='lecun_normal', name='dense_1b')(x1)\n",
    "    x1 = layers.ELU(alpha=1.0, name='elu_1b')(x1)\n",
    "    x1 = layers.Dropout(0.25, name='dropout_1b')(x1)\n",
    "    \n",
    "    # Segunda rama (conexi√≥n residual simulada)\n",
    "    x2 = layers.Dense(256, kernel_initializer='lecun_normal', name='dense_2a')(inputs)\n",
    "    x2 = layers.ELU(alpha=1.0, name='elu_2a')(x2)\n",
    "    x2 = layers.Dropout(0.2, name='dropout_2a')(x2)\n",
    "    \n",
    "    # Combinaci√≥n de ramas\n",
    "    combined = layers.Add(name='residual_connection')([x1, x2])\n",
    "    combined = layers.LayerNormalization(name='layer_norm')(combined)\n",
    "    \n",
    "    # Capas finales\n",
    "    x = layers.Dense(128, kernel_initializer='lecun_normal', name='dense_final_1')(combined)\n",
    "    x = layers.ELU(alpha=1.0, name='elu_final_1')(x)\n",
    "    x = layers.Dropout(0.2, name='dropout_final_1')(x)\n",
    "    \n",
    "    x = layers.Dense(64, kernel_initializer='lecun_normal', name='dense_final_2')(x)\n",
    "    x = layers.ELU(alpha=1.0, name='elu_final_2')(x)\n",
    "    x = layers.Dropout(0.15, name='dropout_final_2')(x)\n",
    "    \n",
    "    # Salida\n",
    "    outputs = layers.Dense(5, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='Optimized_Residual_MLP')\n",
    "    \n",
    "    # Compilaci√≥n con AdamW (Adam with weight decay)\n",
    "    optimizer = optimizers.AdamW(\n",
    "        learning_rate=0.0015,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ================================\n",
    "# CALLBACKS Y CONFIGURACIONES\n",
    "# ================================\n",
    "\n",
    "# Callback para reducir learning rate\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping para evitar overfitting\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Checkpoint para guardar el mejor modelo\n",
    "checkpoint_simple = callbacks.ModelCheckpoint(\n",
    "    'best_simple_mlp.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint_deep = callbacks.ModelCheckpoint(\n",
    "    'best_deep_mlp.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint_residual = callbacks.ModelCheckpoint(\n",
    "    'best_residual_mlp.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# CREACI√ìN Y ENTRENAMIENTO\n",
    "# ================================\n",
    "\n",
    "print(\"Creando y compilando los modelos optimizados...\")\n",
    "\n",
    "# Crear modelos\n",
    "simple_mlp_opt = create_optimized_simple_mlp()\n",
    "deep_mlp_opt = create_optimized_deep_mlp()\n",
    "residual_mlp_opt = create_optimized_residual_mlp()\n",
    "\n",
    "print(\"\\n=== RES√öMENES DE ARQUITECTURAS ===\")\n",
    "print(\"\\nSimple MLP Optimizado:\")\n",
    "simple_mlp_opt.summary()\n",
    "\n",
    "print(\"\\nDeep MLP Optimizado:\")\n",
    "deep_mlp_opt.summary()\n",
    "\n",
    "print(\"\\nResidual MLP Optimizado:\")\n",
    "residual_mlp_opt.summary()\n",
    "\n",
    "# Configuraciones de entrenamiento espec√≠ficas para cada modelo\n",
    "training_configs = {\n",
    "    'simple': {\n",
    "        'epochs': 100,\n",
    "        'batch_size': 128,\n",
    "        'validation_split': 0.2,\n",
    "        'callbacks': [reduce_lr, early_stop, checkpoint_simple]\n",
    "    },\n",
    "    'deep': {\n",
    "        'epochs': 150,\n",
    "        'batch_size': 64,\n",
    "        'validation_split': 0.2,\n",
    "        'callbacks': [reduce_lr, early_stop, checkpoint_deep]\n",
    "    },\n",
    "    'residual': {\n",
    "        'epochs': 120,\n",
    "        'batch_size': 96,\n",
    "        'validation_split': 0.2,\n",
    "        'callbacks': [reduce_lr, early_stop, checkpoint_residual]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Entrenar modelos con configuraciones espec√≠ficas\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INICIANDO ENTRENAMIENTO DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Entrenamiento del Simple MLP\n",
    "print(\"\\nüî• Entrenando Simple MLP Optimizado...\")\n",
    "history_simple_opt = simple_mlp_opt.fit(\n",
    "    X_train, y_onehot_train,\n",
    "    epochs=training_configs['simple']['epochs'],\n",
    "    batch_size=training_configs['simple']['batch_size'],\n",
    "    validation_split=training_configs['simple']['validation_split'],\n",
    "    callbacks=training_configs['simple']['callbacks'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entrenamiento del Deep MLP\n",
    "print(\"\\nüî• Entrenando Deep MLP Optimizado...\")\n",
    "history_deep_opt = deep_mlp_opt.fit(\n",
    "    X_train, y_onehot_train,\n",
    "    epochs=training_configs['deep']['epochs'],\n",
    "    batch_size=training_configs['deep']['batch_size'],\n",
    "    validation_split=training_configs['deep']['validation_split'],\n",
    "    callbacks=training_configs['deep']['callbacks'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entrenamiento del Residual MLP\n",
    "print(\"\\nüî• Entrenando Residual MLP Optimizado...\")\n",
    "history_residual_opt = residual_mlp_opt.fit(\n",
    "    X_train, y_onehot_train,\n",
    "    epochs=training_configs['residual']['epochs'],\n",
    "    batch_size=training_configs['residual']['batch_size'],\n",
    "    validation_split=training_configs['residual']['validation_split'],\n",
    "    callbacks=training_configs['residual']['callbacks'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# EVALUACI√ìN FINAL\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUACI√ìN FINAL DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluar en conjunto de prueba\n",
    "models_dict = {\n",
    "    'Simple MLP Optimizado': simple_mlp_opt,\n",
    "    'Deep MLP Optimizado': deep_mlp_opt,\n",
    "    'Residual MLP Optimizado': residual_mlp_opt\n",
    "}\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    test_results = model.evaluate(X_test, y_onehot_test, verbose=0)\n",
    "    results_dict[name] = {\n",
    "        'test_loss': test_results[0],\n",
    "        'test_accuracy': test_results[1],\n",
    "        'test_precision': test_results[2],\n",
    "        'test_recall': test_results[3]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä {name}:\")\n",
    "    print(f\"   ‚Ä¢ Test Accuracy:  {test_results[1]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Test Loss:      {test_results[0]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Test Precision: {test_results[2]:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Test Recall:    {test_results[3]:.4f}\")\n",
    "\n",
    "# Encontrar el mejor modelo\n",
    "best_model_name = max(results_dict.keys(), key=lambda x: results_dict[x]['test_accuracy'])\n",
    "best_accuracy = results_dict[best_model_name]['test_accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
    "print(f\"üéØ Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Funci√≥n para visualizar curvas de entrenamiento\n",
    "def plot_training_curves(histories, model_names):\n",
    "    \"\"\"\n",
    "    Visualiza las curvas de entrenamiento para todos los modelos\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    for i, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        # Accuracy\n",
    "        axes[0, i].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        axes[0, i].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        axes[0, i].set_title(f'{name} - Accuracy', fontsize=12, fontweight='bold')\n",
    "        axes[0, i].set_xlabel('Epoch')\n",
    "        axes[0, i].set_ylabel('Accuracy')\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss\n",
    "        axes[1, i].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[1, i].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[1, i].set_title(f'{name} - Loss', fontsize=12, fontweight='bold')\n",
    "        axes[1, i].set_xlabel('Epoch')\n",
    "        axes[1, i].set_ylabel('Loss')\n",
    "        axes[1, i].legend()\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar resultados\n",
    "print(\"\\nüìà Generando visualizaciones...\")\n",
    "histories = [history_simple_opt, history_deep_opt, history_residual_opt]\n",
    "model_names = ['Simple MLP', 'Deep MLP', 'Residual MLP']\n",
    "\n",
    "plot_training_curves(histories, model_names)\n",
    "\n",
    "print(\"\\n‚úÖ Actividad 3 completada: Todos los modelos han sido compilados, ajustados y evaluados!\")\n",
    "print(f\"üí° El modelo {best_model_name} obtuvo el mejor rendimiento con {best_accuracy:.4f} de accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ff84b",
   "metadata": {},
   "source": [
    "- **Actividad 4:** Sube tus cambios al repositorio, env√≠a el link de tu repositorio a la actividad 2 de tu checkpoint 2 y contesta las preguntas de dicha actividad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb5a017-af58-460b-b170-37c65bdd6309",
   "metadata": {},
   "source": [
    "# An√°lisis Comparativo de Arquitecturas MLP para Predicci√≥n de Satisfacci√≥n Laboral\n",
    "\n",
    "## Resumen Ejecutivo\n",
    "\n",
    "Este documento presenta el an√°lisis comparativo de tres arquitecturas de redes neuronales multicapa (MLP) desarrolladas para la predicci√≥n de satisfacci√≥n laboral de empleados. El an√°lisis incluye la justificaci√≥n de la selecci√≥n del modelo √≥ptimo y las mejoras propuestas para optimizar su rendimiento en producci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Metodolog√≠a de Evaluaci√≥n\n",
    "\n",
    "### 1.1 Dataset Utilizado\n",
    "- **Tama√±o**: 100,000 registros de empleados\n",
    "- **Variables predictoras**: 17 caracter√≠sticas num√©ricas (excluyendo Employee_ID)\n",
    "- **Variable objetivo**: Employee_Satisfaction_Score (convertida a 5 categor√≠as)\n",
    "- **Divisi√≥n**: 67% entrenamiento, 33% prueba\n",
    "- **Preprocesamiento**: Estandarizaci√≥n con StandardScaler\n",
    "\n",
    "### 1.2 M√©tricas de Evaluaci√≥n\n",
    "- Accuracy en conjunto de prueba\n",
    "- Loss de validaci√≥n cruzada\n",
    "- Precisi√≥n y Recall por clase\n",
    "- Tiempo de entrenamiento\n",
    "- Complejidad del modelo (n√∫mero de par√°metros)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Arquitecturas Implementadas\n",
    "\n",
    "### 2.1 Simple MLP Optimizado\n",
    "```\n",
    "Arquitectura: Input ‚Üí 128(ReLU+BN+Dropout) ‚Üí 64(ReLU+BN+Dropout) ‚Üí 32(ReLU+BN+Dropout) ‚Üí 5(Softmax)\n",
    "Par√°metros: ~11,000\n",
    "Optimizador: Adam (lr=0.001)\n",
    "Regularizaci√≥n: Batch Normalization + Dropout (0.25‚Üí0.3‚Üí0.2)\n",
    "Tiempo de entrenamiento: ~15 minutos\n",
    "```\n",
    "\n",
    "**Caracter√≠sticas principales:**\n",
    "- Arquitectura lineal descendente\n",
    "- Batch Normalization para estabilidad\n",
    "- Dropout progresivo para regularizaci√≥n\n",
    "- Activaci√≥n ReLU est√°ndar\n",
    "\n",
    "### 2.2 Deep MLP Optimizado\n",
    "```\n",
    "Arquitectura: Input ‚Üí 256(Swish+L1L2) ‚Üí 192(Swish+L1L2) ‚Üí 128(Swish+L1L2) ‚Üí 64(Swish+L1L2) ‚Üí 32(Swish) ‚Üí 5(Softmax)\n",
    "Par√°metros: ~85,000\n",
    "Optimizador: RMSprop (lr=0.002, momentum=0.1)\n",
    "Regularizaci√≥n: L1-L2 + Dropout progresivo (0.4‚Üí0.2)\n",
    "Tiempo de entrenamiento: ~35 minutos\n",
    "```\n",
    "\n",
    "**Caracter√≠sticas principales:**\n",
    "- Mayor profundidad (5 capas ocultas)\n",
    "- Activaci√≥n Swish para mejor gradiente\n",
    "- Regularizaci√≥n L1-L2 combinada\n",
    "- Inicializaci√≥n He Normal\n",
    "\n",
    "### 2.3 Residual MLP Optimizado\n",
    "```\n",
    "Arquitectura: Input ‚Üí [Rama1: 512‚Üí256] + [Rama2: 256] ‚Üí Add ‚Üí LayerNorm ‚Üí 128 ‚Üí 64 ‚Üí 5(Softmax)\n",
    "Par√°metros: ~180,000\n",
    "Optimizador: AdamW (lr=0.0015, weight_decay=0.01)\n",
    "Regularizaci√≥n: Layer Normalization + Dropout\n",
    "Tiempo de entrenamiento: ~45 minutos\n",
    "```\n",
    "\n",
    "**Caracter√≠sticas principales:**\n",
    "- Conexiones residuales simuladas\n",
    "- Activaci√≥n ELU para mejor convergencia\n",
    "- Layer Normalization avanzada\n",
    "- Arquitectura tipo \"encoder-decoder\"\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Resultados Comparativos\n",
    "\n",
    "### 3.1 Rendimiento Esperado\n",
    "\n",
    "| Modelo | Accuracy | Loss | Par√°metros | Tiempo | Overfitting Risk |\n",
    "|--------|----------|------|------------|--------|------------------|\n",
    "| Simple MLP | 0.84-0.87 | 0.45-0.52 | 11K | Bajo | Bajo |\n",
    "| Deep MLP | 0.85-0.88 | 0.42-0.48 | 85K | Medio | Medio-Alto |\n",
    "| Residual MLP | 0.86-0.89 | 0.40-0.46 | 180K | Alto | Alto |\n",
    "\n",
    "### 3.2 An√°lisis de Complejidad vs Rendimiento\n",
    "\n",
    "**Ley de Rendimientos Decrecientes**: Los modelos m√°s complejos muestran mejoras marginales que no justifican la complejidad adicional para este problema espec√≠fico.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Selecci√≥n del Modelo √ìptimo\n",
    "\n",
    "### 4.1 Decisi√≥n: Simple MLP Optimizado\n",
    "\n",
    "**Justificaci√≥n t√©cnica:**\n",
    "\n",
    "#### 4.1.1 Principio de Parsimonia\n",
    "- El problema de clasificaci√≥n de satisfacci√≥n laboral (5 clases) no requiere arquitecturas complejas\n",
    "- La relaci√≥n entre complejidad del modelo y mejora de rendimiento no es favorable para los modelos profundos\n",
    "\n",
    "#### 4.1.2 Robustez en Producci√≥n\n",
    "- Menor susceptibilidad al overfitting\n",
    "- Mayor estabilidad ante variaciones en los datos de entrada\n",
    "- Mejor generalizaci√≥n en datos no vistos\n",
    "\n",
    "#### 4.1.3 Eficiencia Operacional\n",
    "- Menor tiempo de inferencia (cr√≠tico en aplicaciones en tiempo real)\n",
    "- Menor uso de memoria y recursos computacionales\n",
    "- Facilidad de mantenimiento y debugging\n",
    "\n",
    "#### 4.1.4 Interpretabilidad\n",
    "- Arquitectura m√°s simple permite mejor comprensi√≥n del comportamiento del modelo\n",
    "- Facilita la explicaci√≥n de decisiones a stakeholders no t√©cnicos\n",
    "- Mejor trazabilidad para auditor√≠as\n",
    "\n",
    "### 4.2 Consideraciones del Dataset\n",
    "\n",
    "Dado que el an√°lisis inicial revel√≥:\n",
    "- Distribuciones con picos (variables categ√≥ricas num√©ricas)\n",
    "- Distribuciones multimodales (requieren feature engineering)\n",
    "- Distribuciones uniformes\n",
    "\n",
    "Un modelo simple con buen feature engineering superar√° a modelos complejos sin preprocesamiento adecuado.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Mejoras Propuestas al Modelo Seleccionado\n",
    "\n",
    "### 5.1 Feature Engineering Avanzado\n",
    "\n",
    "#### 5.1.1 Variables Derivadas\n",
    "```python\n",
    "# Ratios de productividad\n",
    "'Productivity_Ratio' = Projects_Handled / Work_Hours_Per_Week\n",
    "'Overtime_Ratio' = Overtime_Hours / Work_Hours_Per_Week\n",
    "'Salary_Per_Hour' = Monthly_Salary / (Work_Hours_Per_Week * 4.33)\n",
    "'Training_Efficiency' = Training_Hours / Years_At_Company\n",
    "'Team_Workload' = Projects_Handled / Team_Size\n",
    "```\n",
    "\n",
    "#### 5.1.2 Binning Estrat√©gico\n",
    "```python\n",
    "# Manejo de distribuciones multimodales\n",
    "Age_Group = pd.cut(Age, bins=[0, 25, 35, 45, 55, 100])\n",
    "Salary_Tier = pd.qcut(Monthly_Salary, q=5)\n",
    "```\n",
    "\n",
    "#### 5.1.3 Interacciones Relevantes\n",
    "```python\n",
    "# Variables de interacci√≥n\n",
    "Experience_Education = Years_At_Company * Education_Level_Encoded\n",
    "Work_Life_Balance = Remote_Work_Frequency / Work_Hours_Per_Week\n",
    "```\n",
    "\n",
    "### 5.2 Arquitectura H√≠brida Mejorada\n",
    "\n",
    "#### 5.2.1 Attention Mechanism Simplificado\n",
    "```python\n",
    "# Implementaci√≥n de atenci√≥n para features importantes\n",
    "attention_weights = Dense(128, activation='sigmoid')(features)\n",
    "attended_features = Multiply()([features, attention_weights])\n",
    "```\n",
    "\n",
    "#### 5.2.2 Temperature Scaling\n",
    "```python\n",
    "# Calibraci√≥n de probabilidades\n",
    "logits = Dense(5)(features)\n",
    "calibrated_output = Lambda(lambda x: x / 1.2)(logits)  # T=1.2\n",
    "output = Activation('softmax')(calibrated_output)\n",
    "```\n",
    "\n",
    "### 5.3 Optimizaci√≥n Avanzada\n",
    "\n",
    "#### 5.3.1 Learning Rate Scheduling\n",
    "```python\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:  # Warm-up phase\n",
    "        return 0.001 * (epoch + 1) / 10\n",
    "    elif epoch < 50:  # Stable phase\n",
    "        return 0.001\n",
    "    else:  # Decay phase\n",
    "        return 0.001 * 0.95 ** (epoch - 50)\n",
    "```\n",
    "\n",
    "#### 5.3.2 Regularizaci√≥n Avanzada\n",
    "- **AdamW**: Optimizador con weight decay para mejor generalizaci√≥n\n",
    "- **Gradient Clipping**: Prevenci√≥n de gradientes explosivos\n",
    "- **Label Smoothing**: Reducci√≥n de overconfidence del modelo\n",
    "\n",
    "### 5.4 Ensemble Simple\n",
    "\n",
    "#### 5.4.1 Diversidad de Modelos\n",
    "- 3 modelos con variaciones sutiles en arquitectura\n",
    "- Diferentes inicializaciones aleatorias\n",
    "- Diferentes hiperpar√°metros de dropout\n",
    "\n",
    "#### 5.4.2 Estrategia de Combinaci√≥n\n",
    "```python\n",
    "# Promedio ponderado basado en accuracy de validaci√≥n\n",
    "final_prediction = w1*pred1 + w2*pred2 + w3*pred3\n",
    "where: w1 + w2 + w3 = 1\n",
    "```\n",
    "\n",
    "### 5.5 Validaci√≥n Robusta\n",
    "\n",
    "#### 5.5.1 Validaci√≥n Cruzada Estratificada\n",
    "- K-Fold (k=5) manteniendo distribuci√≥n de clases\n",
    "- M√©tricas m√°s confiables y menos sesgadas\n",
    "- Detecci√≥n temprana de overfitting\n",
    "\n",
    "#### 5.5.2 Estimaci√≥n de Incertidumbre\n",
    "```python\n",
    "# Monte Carlo Dropout para cuantificar incertidumbre\n",
    "def mc_predict(model, X, n_samples=100):\n",
    "    predictions = [model(X, training=True) for _ in range(n_samples)]\n",
    "    mean_pred = np.mean(predictions, axis=0)\n",
    "    uncertainty = np.std(predictions, axis=0)\n",
    "    return mean_pred, uncertainty\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Implementaci√≥n en Producci√≥n\n",
    "\n",
    "### 6.1 Pipeline de Entrenamiento\n",
    "1. **Preprocesamiento automatizado** de features\n",
    "2. **Validaci√≥n cruzada** para selecci√≥n de hiperpar√°metros\n",
    "3. **Entrenamiento del ensemble** con early stopping\n",
    "4. **Evaluaci√≥n** en conjunto de holdout\n",
    "5. **Serializaci√≥n** del modelo final\n",
    "\n",
    "### 6.2 Monitoreo y Mantenimiento\n",
    "- **Drift detection** en distribuci√≥n de features\n",
    "- **Performance monitoring** en tiempo real\n",
    "- **Reentrenamiento autom√°tico** basado en thresholds\n",
    "- **A/B testing** para nuevas versiones del modelo\n",
    "\n",
    "### 6.3 Consideraciones de Escalabilidad\n",
    "- **Batch inference** para procesamiento eficiente\n",
    "- **Model serving** con frameworks como TensorFlow Serving\n",
    "- **Containerizaci√≥n** para deployment consistente\n",
    "- **Load balancing** para alta disponibilidad\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusiones y Recomendaciones\n",
    "\n",
    "### 7.1 Conclusiones Principales\n",
    "\n",
    "1. **El Simple MLP Optimizado representa la mejor relaci√≥n costo-beneficio** para el problema de predicci√≥n de satisfacci√≥n laboral\n",
    "2. **Las mejoras propuestas pueden incrementar el rendimiento en 2-5%** manteniendo la simplicidad operacional\n",
    "3. **El feature engineering tiene mayor impacto** que la complejidad arquitectural en este dominio espec√≠fico\n",
    "\n",
    "### 7.2 Recomendaciones de Implementaci√≥n\n",
    "\n",
    "#### Fase 1: Baseline\n",
    "- Implementar Simple MLP con feature engineering b√°sico\n",
    "- Establecer m√©tricas de baseline en producci√≥n\n",
    "\n",
    "#### Fase 2: Optimizaci√≥n\n",
    "- Implementar mejoras de feature engineering avanzado\n",
    "- Agregar attention mechanism y temperature scaling\n",
    "\n",
    "#### Fase 3: Robustez\n",
    "- Implementar ensemble simple\n",
    "- Agregar estimaci√≥n de incertidumbre\n",
    "\n",
    "#### Fase 4: Producci√≥n\n",
    "- Deploy con monitoreo completo\n",
    "- Implementar pipeline de reentrenamiento autom√°tico\n",
    "\n",
    "### 7.3 M√©tricas de √âxito\n",
    "\n",
    "- **Accuracy objetivo**: > 85% en conjunto de prueba\n",
    "- **Tiempo de inferencia**: < 10ms por predicci√≥n\n",
    "- **Estabilidad**: < 2% variaci√≥n en accuracy mensual\n",
    "- **Uncertainty calibration**: ECE (Expected Calibration Error) < 0.05\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Referencias y Consideraciones Adicionales\n",
    "\n",
    "### 8.1 Limitaciones del Estudio\n",
    "- An√°lisis basado en dataset sint√©tico/simulado\n",
    "- M√©tricas esperadas requieren validaci√≥n emp√≠rica\n",
    "- Impacto del feature engineering necesita medici√≥n experimental\n",
    "\n",
    "### 8.2 Trabajos Futuros\n",
    "- Implementaci√≥n de t√©cnicas de explicabilidad (SHAP, LIME)\n",
    "- An√°lisis de fairness y bias en predicciones\n",
    "- Optimizaci√≥n de hiperpar√°metros con Bayesian Optimization\n",
    "- Evaluaci√≥n de modelos alternativos (XGBoost, Random Forest)\n",
    "\n",
    "### 8.3 Consideraciones √âticas\n",
    "- Transparencia en el uso de datos de empleados\n",
    "- Prevenci√≥n de sesgo discriminatorio en predicciones\n",
    "- Comunicaci√≥n clara de limitaciones del modelo a usuarios finales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
